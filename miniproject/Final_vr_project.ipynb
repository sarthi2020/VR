{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Preprocessing Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dataset'\n",
    "categories = os.listdir(data_path)\n",
    "labels = [i for i in range(len(categories))]\n",
    "label_dict = dict(zip(categories,labels))\n",
    "\n",
    "img_size = 100\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(data_path,category)\n",
    "    img_names = os.listdir(folder_path)\n",
    "        \n",
    "    for img_name in img_names:\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        img=cv2.imread(img_path)\n",
    "        gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)           \n",
    "        resized=cv2.resize(gray,(img_size,img_size))\n",
    "        data.append(resized)\n",
    "        target.append(label_dict[category])\n",
    "\n",
    "            \n",
    "data = np.array(data)/255.0\n",
    "data = np.reshape(data,(data.shape[0],img_size,img_size,1))\n",
    "target = np_utils.to_categorical(np.array(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 256)       2560      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 98, 98, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 47, 47, 128)       295040    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 47, 47, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                3612736   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 4,058,050\n",
      "Trainable params: 4,058,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model.add(Conv2D(256,(3,3),input_shape=data.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional layer 2\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional layer 3\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# FC layer 1\n",
    "model.add(Dense(64,activation='relu'))\n",
    "\n",
    "# FC layer 2\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31/31 [==============================] - 53s 2s/step - loss: 0.6952 - accuracy: 0.5495 - val_loss: 0.6318 - val_accuracy: 0.6048\n",
      "Epoch 2/20\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.5615 - accuracy: 0.6929 - val_loss: 0.4561 - val_accuracy: 0.7863\n",
      "Epoch 3/20\n",
      "31/31 [==============================] - 52s 2s/step - loss: 0.3127 - accuracy: 0.8636 - val_loss: 0.6911 - val_accuracy: 0.6855\n",
      "Epoch 4/20\n",
      "31/31 [==============================] - 57s 2s/step - loss: 0.2721 - accuracy: 0.9051 - val_loss: 0.1266 - val_accuracy: 0.9476\n",
      "Epoch 5/20\n",
      "31/31 [==============================] - 51s 2s/step - loss: 0.1218 - accuracy: 0.9545 - val_loss: 0.1204 - val_accuracy: 0.9597\n",
      "Epoch 6/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0817 - accuracy: 0.9717 - val_loss: 0.1395 - val_accuracy: 0.9355\n",
      "Epoch 7/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0616 - accuracy: 0.9778 - val_loss: 0.1170 - val_accuracy: 0.9556\n",
      "Epoch 8/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0355 - accuracy: 0.9869 - val_loss: 0.0906 - val_accuracy: 0.9637\n",
      "Epoch 9/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0204 - accuracy: 0.9929 - val_loss: 0.1250 - val_accuracy: 0.9597\n",
      "Epoch 10/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0297 - accuracy: 0.9889 - val_loss: 0.1311 - val_accuracy: 0.9597\n",
      "Epoch 11/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0553 - accuracy: 0.9758 - val_loss: 0.1138 - val_accuracy: 0.9597\n",
      "Epoch 12/20\n",
      "31/31 [==============================] - 50s 2s/step - loss: 0.0287 - accuracy: 0.9939 - val_loss: 0.1200 - val_accuracy: 0.9597\n",
      "Epoch 13/20\n",
      "31/31 [==============================] - 50s 2s/step - loss: 0.0127 - accuracy: 0.9949 - val_loss: 0.0948 - val_accuracy: 0.9718\n",
      "Epoch 14/20\n",
      "31/31 [==============================] - 50s 2s/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.1194 - val_accuracy: 0.9556\n",
      "Epoch 15/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 0.9677\n",
      "Epoch 16/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1299 - val_accuracy: 0.9677\n",
      "Epoch 17/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 6.5823e-04 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9637\n",
      "Epoch 18/20\n",
      "31/31 [==============================] - 50s 2s/step - loss: 3.8900e-04 - accuracy: 1.0000 - val_loss: 0.1450 - val_accuracy: 0.9637\n",
      "Epoch 19/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.1460 - val_accuracy: 0.9637\n",
      "Epoch 20/20\n",
      "31/31 [==============================] - 49s 2s/step - loss: 0.0175 - accuracy: 0.9919 - val_loss: 0.1178 - val_accuracy: 0.9556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f6dce6fc40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "train_data,test_data,train_target,test_target = train_test_split(data,target,train_size=0.9)\n",
    "model.fit(train_data,train_target,epochs=20,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CNN, Haarcascade and Yolo on live webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "yolo_net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "outputlayers = [layer_names[i[0] - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "print_dict={0:'Allowed',1:'Not Allowed'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv2.VideoCapture(0)\n",
    "\n",
    "starting_time= time.time()\n",
    "frame_id = 0\n",
    "\n",
    "while(True):\n",
    "    _,frame= source.read() \n",
    "    frame_id+=1\n",
    "    img = frame\n",
    "    flag = 0\n",
    "    \n",
    "    height,width,channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame,0.00392,(320,320),(0,0,0),True,crop=False) \n",
    "\n",
    "    yolo_net.setInput(blob)\n",
    "    outs = yolo_net.forward(outputlayers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0]*width)\n",
    "                center_y = int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "\n",
    "                x = int(center_x - w/2)\n",
    "                y = int(center_y - h/2)\n",
    "                boxes.append([x,y,w,h])\n",
    "                confidences.append(float(confidence)) \n",
    "                class_ids.append(class_id) \n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n",
    "\n",
    "    for i in indexes:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        if(class_ids[i] == 0):\n",
    "            flag = 1\n",
    "            cv2.rectangle(frame, (round(box[0]),round(box[1])), (round(box[0]+box[2]),round(box[1]+box[3])), (0, 0, 0), 2)\n",
    "            cv2.putText(frame,'person', (round(box[0])-10,round(box[1])-10),font, 0.5, (0, 0, 0), 2)\n",
    "\n",
    "    if(flag == 1):\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_classifier.detectMultiScale(gray,1.1,4)  \n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "            face_img = gray[y:y+w,x:x+w]\n",
    "            resized = cv2.resize(face_img,(100,100))\n",
    "            normalized = resized/255.0\n",
    "            reshaped = np.reshape(normalized,(1,100,100,1))\n",
    "            result = model.predict(reshaped)\n",
    "\n",
    "            label = np.argmax(result,axis=1)[0]\n",
    "\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "            cv2.putText(img, print_dict[label],(x, y-10),font,0.5,(255,255,255),2)\n",
    "\n",
    "\n",
    "    elapsed_time = time.time() - starting_time\n",
    "    fps = frame_id/elapsed_time\n",
    "    cv2.putText(frame,\"FPS:\"+str(round(fps,2)),(10,50),cv2.FONT_HERSHEY_PLAIN,2,(0,0,0),1)\n",
    "         \n",
    "    cv2.imshow('LIVE',img)\n",
    "\n",
    "    key = cv2.waitKey(1)           # press escape key to exit webcam\n",
    "    if(key == 27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
